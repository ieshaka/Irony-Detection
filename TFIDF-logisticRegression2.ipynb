{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocessor as p\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "data = pd.read_csv('train.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean the tweets \n",
    "def clean_docs(tweet_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.SMILEY)\n",
    "    cleaned_text = []\n",
    "    for i in tweet_list:\n",
    "        cleaned_text.append(p.clean(i))\n",
    "        \n",
    "    stop_word_cleared = []  \n",
    "    for j in cleaned_text:\n",
    "        word_tokens = j.split()\n",
    "        filtered_sentence = ''\n",
    "\n",
    "        for w in word_tokens: \n",
    "            if w not in stop_words: \n",
    "                w = lemmatizer.lemmatize(w)\n",
    "                filtered_sentence = filtered_sentence + ' ' +w.lower()\n",
    "        stop_word_cleared.append(filtered_sentence)\n",
    "    return stop_word_cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating into irony and non irony \n",
    "irony = []\n",
    "non_irony = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['Label'][i] == 1:\n",
    "        irony.append(data['Tweet text'][i])\n",
    "    else:\n",
    "        non_irony.append(data['Tweet text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating into test and training \n",
    "irony_train = irony[:-382]\n",
    "irony_test = irony[-382:]\n",
    "non_irony_train = non_irony[:-382]\n",
    "non_irony_test = non_irony[-382:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of test and training \n",
    "Xtrain = irony_train + non_irony_train\n",
    "Xtest = irony_test + non_irony_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test and training labels \n",
    "ytrain = array([1 for _ in range(1519)] + [0 for _ in range(1534)])\n",
    "ytest = array([1 for _ in range(382)] + [0 for _ in range(382)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_cleaned = clean_docs(Xtrain)\n",
    "Xtest_cleaned = clean_docs(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a tfidf vector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(Xtrain_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfifd = tfidf_vectorizer.transform(Xtest_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7257203842049094"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression model using BOW as feature \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(train_tfidf, ytrain) # training the model\n",
    "\n",
    "prediction = lreg.predict_proba(test_tfifd) # predicting on the validation set\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "f1_score(ytest, prediction_int) # calculating f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the test data\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the test data\n",
    "testset_cleaned = clean_docs(test_data['Tweet text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = tfidf_vectorizer.transform(testset_cleaned)\n",
    "test_pred = lreg.predict_proba(tfidf_test)\n",
    "test_pred_int = test_pred[:,1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "test_data['Label'] = test_pred_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_py35",
   "language": "python",
   "name": "msc_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
